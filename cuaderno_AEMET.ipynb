{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the extraction of the data from the AEMET API and saving it to our own csv for transforming.\n",
    "The GET method for the stations returns an url with a json format that we can read with the urllib library. \n",
    "Check the decode function since otherwise it returns an error with the coding (utf-8 didn't work either)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "url = \"https://opendata.aemet.es/opendata/api/valores/climatologicos/inventarioestaciones/todasestaciones\"\n",
    "\n",
    "#This method allows us to access the .env file and load our env variables\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ.get('api_key')\n",
    "path = os.environ.get('path')\n",
    "\n",
    "querystring = {\"api_key\":api_key}\n",
    "\n",
    "headers = {\n",
    "    'cache-control': \"no-cache\"\n",
    "    }\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "if response.ok:\n",
    "    data = response.json()\n",
    "    print(data['datos'])\n",
    "\n",
    "\n",
    "# store the URL in url as \n",
    "# parameter for urlopen\n",
    "url = data['datos']\n",
    "  \n",
    "# store the response of URL\n",
    "response = urlopen(url)\n",
    "  \n",
    "# storing the JSON response \n",
    "# from url in data\n",
    "data_json = json.loads(response.read().decode('ISO-8859-1'))\n",
    "  \n",
    "# moving data to csv\n",
    "\n",
    "table_stations = pd.json_normalize(data_json)\n",
    "table_stations.to_csv(f\"{path}estaciones.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "from datetime import date, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import dask.dataframe as da\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import uuid\n",
    "\n",
    "\n",
    "#This function will allow us to customize the name of the parquet files\n",
    "\n",
    "def batch_id(n):\n",
    "    id = uuid.uuid4()\n",
    "    return f\"part-{n}-{id}.parquet\"\n",
    "\n",
    "table = pd.DataFrame()\n",
    "\n",
    "\n",
    "def save_data(data):\n",
    "\n",
    "    # store the URL in url as \n",
    "    # parameter for urlopen\n",
    "    # the data has to have 4 fields; otherwise the API is sending back an error\n",
    "\n",
    "    if len(data) == 4:\n",
    "        url = data['datos']\n",
    "        \n",
    "        # store the response of URL\n",
    "\n",
    "        try:\n",
    "            response = urllib.request.urlopen(url)\n",
    "            if str(response.code).startswith('2'):\n",
    "            # storing the JSON response \n",
    "            # from url in data\n",
    "                data_json = json.loads(response.read().decode('ISO-8859-1'))\n",
    "                aux_table = pd.json_normalize(data_json)\n",
    "                return aux_table\n",
    "        except urllib.error.HTTPError as err:\n",
    "            print(f'A HTTPError was thrown: {err.code} {err.reason}')\n",
    "            #usually the main error is a 500, so we wait because we're doing too many requests to the server\n",
    "            time.sleep(120)\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def data_extract():\n",
    "    global table\n",
    "    data = response.json()\n",
    "    aux_table = save_data(data)\n",
    "    #This allow you to see what's going on, especially in long executions such as extracting the historical data\n",
    "    print(\"month: \" + str(single_date.month) + \" |--| table size:\" + str(table.shape))\n",
    "    if aux_table is not None:\n",
    "        if aux_table.shape[0] > 0:\n",
    "            table = pd.concat([table, aux_table])\n",
    "\n",
    "\n",
    "#We already accessed our .env variable in the first cell of the notebook\n",
    "#Nonetheless, we might want to just execute this part of the code and not extract the whole catalogue of stations\n",
    "#This method allows us to access the .env file and load our env variables\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ.get('api_key')\n",
    "path = os.environ.get('path')\n",
    "\n",
    "querystring = {\"api_key\":api_key}\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'cache-control': \"no-cache\"\n",
    "    }\n",
    "\n",
    "\n",
    "#We're going to extract the historical data and create a file per year\n",
    "year_df = pd.date_range(start='1/1/1920', end='10/1/2023', freq='YS')  \n",
    "for single_year in year_df:\n",
    "    \n",
    "    start_year = single_year\n",
    "    end_year = single_year + relativedelta(years = 1)- timedelta(days = 1)\n",
    "\n",
    "    d = pd.date_range(start=str(start_year.date()), end=str(end_year.date()), freq='MS') \n",
    "    \n",
    "    for single_date in d:\n",
    "        start_date = single_date\n",
    "        end_date = single_date + relativedelta(months = 1) - timedelta(days = 1)\n",
    "\n",
    "        url = \"https://opendata.aemet.es/opendata/api/valores/climatologicos/diarios/datos/fechaini/\" + start_date.strftime(\"%Y-%m-%dT%H%%3A%M%%3A%SUTC\") + \"/fechafin/\" + end_date.strftime(\"%Y-%m-%dT23%%3A59%%3A59UTC\") + \"/todasestaciones\"\n",
    "        \n",
    "        response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "        if response.ok:\n",
    "           data_extract()\n",
    "        else:\n",
    "            print(response.reason)\n",
    "            time.sleep(60)\n",
    "            response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "            if response.ok:\n",
    "                data_extract()\n",
    "            else:\n",
    "                sys.exit(\"Exit process - not completed\")\n",
    "    \n",
    "    time.sleep(15)\n",
    "        \n",
    " \n",
    "    ddf = da.from_pandas(table, chunksize=150000)\n",
    "    save_dir = f\"{path}parquets/\"\n",
    "    \n",
    "    #We use this lambda to create a custom filename for our parquet files\n",
    "    name_function = lambda x: f\"data-{single_year.year}.parquet\"  \n",
    "    \n",
    "    ddf.to_parquet(save_dir,name_function=name_function)\n",
    "    table.drop(table.index , inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want to update weekly our data, so we're doing something similar\n",
    "#to the historical data process but relative to the current date\n",
    "#DO NOT USE THIS CODE JUST YET\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "import sys\n",
    "\n",
    "import time\n",
    "\n",
    "from datetime import date, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import dask.dataframe as da\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "table = pd.DataFrame()\n",
    "\n",
    "\n",
    "def save_data(data):\n",
    "\n",
    "    # store the URL in url as \n",
    "    # parameter for urlopen\n",
    "    # the data has to have 4 fields; otherwise the API is sending back an error\n",
    "\n",
    "    if len(data) == 4:\n",
    "        url = data['datos']\n",
    "        \n",
    "        # store the response of URL\n",
    "\n",
    "        try:\n",
    "            response = urllib.request.urlopen(url)\n",
    "        except urllib.error.HTTPError as err:\n",
    "            print(f'A HTTPError was thrown: {err.code} {err.reason}')\n",
    "            #usually the main error is a 500, so we wait because we're doing too many requests to the server\n",
    "            time.sleep(60)\n",
    "\n",
    "\n",
    "        if str(response.code).startswith('2'):\n",
    "            # storing the JSON response \n",
    "            # from url in data\n",
    "            data_json = json.loads(response.read().decode('ISO-8859-1'))\n",
    "            aux_table = pd.json_normalize(data_json)\n",
    "            return aux_table\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def data_extract():\n",
    "    global table\n",
    "    data = response.json()\n",
    "    aux_table = save_data(data)\n",
    "    #print(\"month: \" + str(single_date.month) + \" |--| table size:\" + str(table.shape))\n",
    "    if aux_table is not None:\n",
    "        if aux_table.shape[0] > 0:\n",
    "            table = pd.concat([table, aux_table])\n",
    "\n",
    "\n",
    "#We already accessed our .env variable in the first cell of the notebook\n",
    "#Nonetheless, we might want to just execute this part of the code and not extract the whole catalogue of stations\n",
    "#This method allows us to access the .env file and load our env variables\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ.get('api_key')\n",
    "path = os.environ.get('path')\n",
    "\n",
    "querystring = {\"api_key\":api_key}\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'cache-control': \"no-cache\"\n",
    "    }\n",
    "\n",
    "\n",
    "#OJO PORQUE SOBREESCRIBE LOS PARQUETS QUE HAY\n",
    "#url = \"https://opendata.aemet.es/opendata/api/valores/climatologicos/diarios/datos/fechaini/\" + start_date.strftime(\"%Y-%m-%dT%H%%3A%M%%3A%SUTC\") + \"/fechafin/\" + end_date.strftime(\"%Y-%m-%dT23%%3A59%%3A59UTC\") + \"/todasestaciones\"\n",
    "url = \"https://opendata.aemet.es/opendata/api/valores/climatologicos/diarios/datos/fechaini/2023-09-26T00:00:00UTC/fechafin/2023-10-01T23:59:59UTC/todasestaciones\"\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "if response.ok:\n",
    "    data_extract()\n",
    "else:\n",
    "    print(response.reason)\n",
    "    time.sleep(60)\n",
    "    response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "    if response.ok:\n",
    "        data_extract()\n",
    "    else:\n",
    "        sys.exit(\"Exit process - not completed\")\n",
    "\n",
    "#file_name = path + str(single_year.year) + \"datos_climatologicos.parquet\"\n",
    "#file_name = path + \"nuevos_datos_climatologicos.parquet\"\n",
    "table.to_csv(f\"{path}parquets/nuevos2/prueba2.csv\", sep=\";\", index=False)\n",
    "\n",
    "aux_df = pd.read_csv(f\"{path}parquets/nuevos2/prueba2.csv\", sep=\";\", index_col=False)\n",
    "\n",
    "#ddf = da.from_pandas(table, chunksize=50000)\n",
    "ddf = da.from_pandas(aux_df, chunksize=50000)\n",
    "save_dir = f\"{path}parquets/nuevos2\"\n",
    "ddf.to_parquet(save_dir)\n",
    "table.drop(table.index , inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading the parquets to Google Cloud\n",
    "# We're using the transfer_manager method\n",
    "\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from google.cloud.storage import Client, transfer_manager\n",
    "\n",
    "# First, you have to install the google cloud sdk -> https://cloud.google.com/sdk/docs/install\n",
    "# Second, you have to initialize it and log in (manually)\n",
    "# You also have to set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of your json file\n",
    "\n",
    "# When you're initializing the whole thing it will ask you to select your project, keep that information at hand\n",
    "# You have to have permissions in Google Cloud for your user, so if it crashes because of it, \n",
    "# go to the Cloud Console and create/ the user to give it the role you need\n",
    "\n",
    "def upload_directory_with_transfer_manager(bucket_name, source_directory):\n",
    "    \"\"\"Upload every file in a directory, including all files in subdirectories.\n",
    "\n",
    "    Each blob name is derived from the filename, not including the `directory`\n",
    "    parameter itself. For complete control of the blob name for each file (and\n",
    "    other aspects of individual blob metadata), use\n",
    "    transfer_manager.upload_many() instead.\n",
    "    \"\"\"\n",
    "\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "    # The directory on your computer to upload. Files in the directory and its\n",
    "    # subdirectories will be uploaded. An empty string means \"the current\n",
    "    # working directory\".\n",
    "    # source_directory=\"\"\n",
    "\n",
    "    #HAD TO REMOVE THIS IN JUPYTER, DOESN'T WORK WITH MULTIPROCESSING\n",
    "    # The maximum number of processes to use for the operation. The performance\n",
    "    # impact of this value depends on the use case, but smaller files usually\n",
    "    # benefit from a higher number of processes. Each additional process occupies\n",
    "    # some CPU and memory resources until finished. Threads can be used instead\n",
    "    # of processes by passing `worker_type=transfer_manager.THREAD`.\n",
    "    # workers=8\n",
    "\n",
    "    storage_client = Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Generate a list of paths (in string form) relative to the `directory`.\n",
    "    # This can be done in a single list comprehension, but is expanded into\n",
    "    # multiple lines here for clarity.\n",
    "\n",
    "    # First, recursively get all files in `directory` as Path objects.\n",
    "    directory_as_path_obj = Path(source_directory)\n",
    "    paths = directory_as_path_obj.rglob(\"*\")\n",
    "\n",
    "    # Filter so the list only includes files, not directories themselves.\n",
    "    file_paths = [path for path in paths if path.is_file()]\n",
    "\n",
    "    # These paths are relative to the current working directory. Next, make them\n",
    "    # relative to `directory`\n",
    "    relative_paths = [path.relative_to(source_directory) for path in file_paths]\n",
    "\n",
    "    # Finally, convert them all to strings.\n",
    "    string_paths = [str(path) for path in relative_paths]\n",
    "\n",
    "    print(\"Found {} files.\".format(len(string_paths)))\n",
    "\n",
    "    # Start the upload.\n",
    "    results = transfer_manager.upload_many_from_filenames(\n",
    "        bucket, string_paths, source_directory=source_directory\n",
    "    )\n",
    "\n",
    "    for name, result in zip(string_paths, results):\n",
    "        # The results list is either `None` or an exception for each filename in\n",
    "        # the input list, in order.\n",
    "\n",
    "        if isinstance(result, Exception):\n",
    "            print(\"Failed to upload {} due to exception: {}\".format(name, result))\n",
    "        else:\n",
    "            print(\"Uploaded {} to {}.\".format(name, bucket.name))\n",
    "\n",
    "\n",
    "# To begin the transfer, we need access to the .json file that allows us \n",
    "# to connect your script to Google Cloud Platform. Access the .env file to obtain the route\n",
    "# and also the name of the bucket where you want to load your parquet files\n",
    "\n",
    "file = os.environ['GOOGLE_APPLICATION_CREDENTIALS']  \n",
    "directory_in_str = os.environ['path_parquet']\n",
    "bucket_name = os.environ['bucket_name']\n",
    "upload_directory_with_transfer_manager(bucket_name, directory_in_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4075207 rows.\n"
     ]
    }
   ],
   "source": [
    "# We have our data stored in the Google Cloud\n",
    "# Now we move it to BQ\n",
    "# First, if you haven't already, give access to your user in BQ by \"Sharing\" in your project and adding your user/role\n",
    "# Then, you can execute this script. In the .env store your uri and your BigQuery table ID\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client()\n",
    "table_id = os.environ.get('table_id')\n",
    "uri = os.environ.get('uri')\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    source_format=bigquery.SourceFormat.PARQUET,\n",
    ")\n",
    "\n",
    "load_job = client.load_table_from_uri(\n",
    "    uri, table_id, job_config=job_config\n",
    ")  # Make an API request.\n",
    "\n",
    "load_job.result()  # Waits for the job to complete.\n",
    "\n",
    "destination_table = client.get_table(table_id)\n",
    "print(\"Loaded {} rows.\".format(destination_table.num_rows))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
